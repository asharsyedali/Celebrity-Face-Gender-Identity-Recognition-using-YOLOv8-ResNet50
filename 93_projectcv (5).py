# -*- coding: utf-8 -*-
"""93_projectCV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a5AASPUoDFTxnn0fUvlwnvLl3PjAYcut
"""



from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_path = '/content/drive/MyDrive/s-2025-multi-class-pretraied-network-project.zip'
extract_path = '/content/data'  # All data will be extracted here

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction complete!")
print("Extracted folders:", os.listdir(extract_path))

!pip install -q albumentations==1.4.3

# --- Imports ---
from torchvision import datasets
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
import albumentations as A
from albumentations.pytorch import ToTensorV2
from PIL import Image
import numpy as np

# --- Custom Dataset Wrapper for Albumentations ---
class AlbumentationsDataset(Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __getitem__(self, idx):
        img, label = self.dataset[idx]
        img = np.array(img)
        if self.transform:
            img = self.transform(image=img)["image"]
        return img, label

    def __len__(self):
        return len(self.dataset)

# --- Ultra-strong Augmentation for Training ---
train_aug = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.7),
    A.VerticalFlip(p=0.15),
    A.ShiftScaleRotate(shift_limit=0.09, scale_limit=0.14, rotate_limit=25, p=0.7),
    A.RandomBrightnessContrast(p=0.7),
    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=18, val_shift_limit=10, p=0.5),
    A.GaussianBlur(blur_limit=(3, 7), p=0.15),
    A.ImageCompression(quality_lower=60, quality_upper=100, p=0.2),
    A.CLAHE(p=0.1),
    A.Posterize(num_bits=4, p=0.1),
    A.RandomResizedCrop(height=224, width=224, scale=(0.75, 1.0), ratio=(0.9, 1.1), p=0.7),  # <-- Fixed syntax
    A.CoarseDropout(max_holes=1, max_height=40, max_width=40, min_holes=1, min_height=10, min_width=10, fill_value=0, p=0.3),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2(),
])

# --- Light Augmentation for Validation ---
val_aug = A.Compose([
    A.Resize(224, 224),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2(),
])

# --- Prepare dataset paths ---
data_dir = '/content/data/Train'
full_dataset = datasets.ImageFolder(data_dir)

# --- Stratified split ---
targets = np.array([y for _, y in full_dataset.samples])
indices = np.arange(len(full_dataset))
train_idx, val_idx = train_test_split(indices, test_size=0.15, stratify=targets, random_state=42)
train_base = Subset(full_dataset, train_idx)
val_base = Subset(full_dataset, val_idx)

# --- Wrap with AlbumentationsDataset ---
train_dataset = AlbumentationsDataset(train_base, transform=train_aug)
val_dataset = AlbumentationsDataset(val_base, transform=val_aug)

# --- Data loaders (batch size 16 for best accuracy) ---
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)

# --- Show class mapping and sample counts ---
print("Class-to-index mapping:", full_dataset.class_to_idx)
print(f"Train images: {len(train_dataset)} | Validation images: {len(val_dataset)}")

import torchvision
import torch.nn as nn
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load ResNet50 pretrained on ImageNet
model = torchvision.models.resnet50(pretrained=True)

# Freeze 90% of layers (we'll fine-tune more later!)
total_params = len(list(model.parameters()))
for i, param in enumerate(model.parameters()):
    if i < int(total_params * 0.9):
        param.requires_grad = False

# Custom 10%+ head (bigger than default!)
model.fc = nn.Sequential(
    nn.Linear(model.fc.in_features, 512),
    nn.BatchNorm1d(512),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(512, 128),
    nn.BatchNorm1d(128),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(128, 16)  # For your 16 classes
)

model = model.to(device)

# Print summary
print("Model ready! Trainable parameters:",
      sum(p.numel() for p in model.parameters() if p.requires_grad))

import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

# Only update trainable parameters (head, some last layers)
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)

# Use label smoothing to boost F1!
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

# Learning rate scheduler
scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.3, verbose=True)

# Early stopping setup
best_top1 = 0
patience = 6
patience_counter = 0

import numpy as np
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, top_k_accuracy_score

def evaluate(model, loader):
    model.eval()
    total_loss = 0
    all_preds, all_labels, all_probs = [], [], []
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * imgs.size(0)
            probs = torch.softmax(outputs, dim=1).cpu().numpy()
            preds = np.argmax(probs, axis=1)
            all_preds.append(preds)
            all_labels.append(labels.cpu().numpy())
            all_probs.append(probs)
    all_preds = np.concatenate(all_preds)
    all_labels = np.concatenate(all_labels)
    all_probs = np.concatenate(all_probs)
    avg_loss = total_loss / len(loader.dataset)
    top1_acc = accuracy_score(all_labels, all_preds)
    top3_acc = top_k_accuracy_score(all_labels, all_probs, k=3, labels=np.arange(16))
    f1 = f1_score(all_labels, all_preds, average='weighted')
    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    return avg_loss, top1_acc, top3_acc, f1, precision, recall

train_losses = []
val_losses = []
val_top1 = []
val_f1 = []

epochs = 35

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * imgs.size(0)
    train_loss = running_loss / len(train_loader.dataset)

    # Validation metrics
    val_loss, top1, top3, f1, precision, recall = evaluate(model, val_loader)
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    val_top1.append(top1)
    val_f1.append(f1)

    print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Top-1 Acc: {top1:.4f} | Top-3 Acc: {top3:.4f} | F1: {f1:.4f} | Prec: {precision:.4f} | Recall: {recall:.4f}")

    # Scheduler and early stopping
    scheduler.step(top1)
    if top1 > best_top1:
        best_top1 = top1
        patience_counter = 0
        torch.save(model.state_dict(), "best_model.pth")
    else:
        patience_counter += 1
        if patience_counter > patience:
            print(f"Early stopping at epoch {epoch+1}!")
            break

print("Training complete!")

# === Fine-tuning phase ===

# Load best model weights so far
model.load_state_dict(torch.load("best_model.pth"))

# Unfreeze more: layer3, layer4, and fc head
for name, param in model.named_parameters():
    if "layer3" in name or "layer4" in name or "fc" in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

# Redefine optimizer with lower LR for fine-tuning
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)

# (Re-)initialize lists for plotting fine-tuning curves
ft_train_losses = []
ft_val_losses = []
ft_val_top1 = []
ft_val_f1 = []

# Fine-tuning early stopping variables (separate from pretraining)
ft_best_top1 = 0
ft_patience = 6
ft_patience_counter = 0

epochs = 25

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * imgs.size(0)
    train_loss = running_loss / len(train_loader.dataset)

    # Validation metrics
    val_loss, top1, top3, f1, precision, recall = evaluate(model, val_loader)
    ft_train_losses.append(train_loss)
    ft_val_losses.append(val_loss)
    ft_val_top1.append(top1)
    ft_val_f1.append(f1)

    print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Top-1 Acc: {top1:.4f} | Top-3 Acc: {top3:.4f} | F1: {f1:.4f} | Prec: {precision:.4f} | Recall: {recall:.4f}")

    # Scheduler and early stopping
    scheduler.step(top1)
    if top1 > ft_best_top1:
        ft_best_top1 = top1
        ft_patience_counter = 0  # Reset patience on improvement
        torch.save(model.state_dict(), "best_model.pth")
    else:
        ft_patience_counter += 1
        if ft_patience_counter > ft_patience:
            print(f"Early stopping at epoch {epoch+1}!")
            break

print("Fine-tuning complete!")

import os
from torchvision import transforms

from PIL import Image
from tqdm import tqdm
import pandas as pd

# Load your best model weights
model.load_state_dict(torch.load("best_model.pth"))
model.eval()

test_dir = '/content/data/Pridect'
test_images = sorted([f for f in os.listdir(test_dir) if f.lower().endswith(('.jpg', '.png'))])

# Use validation transform (no augmentation)
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Map indices back to class names
class_to_idx = full_dataset.class_to_idx
idx_to_class = {v: k for k, v in class_to_idx.items()}

filenames = []
mapped_labels = []

with torch.no_grad():
    for fname in tqdm(test_images):
        img = Image.open(os.path.join(test_dir, fname)).convert('RGB')
        input_tensor = test_transform(img).unsqueeze(0).to(device)
        output = model(input_tensor)
        pred = output.argmax(dim=1).item()
        label_str = idx_to_class[pred]  # Map predicted index to label
        filenames.append(fname)
        mapped_labels.append(label_str)

# === Save to submission.csv ===
submission_df = pd.DataFrame({'Id': filenames, 'label': mapped_labels})
submission_df.to_csv("submission.csv", index=False)

print(f"✅ Submission saved: submission.csv with {len(submission_df)} rows")
print(submission_df.head())

print("Train Losses:", train_losses)
print("Val Losses:", val_losses)
print("Val Top-1 Acc:", val_top1)
print("Val F1:", val_f1)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# === INITIAL TRAINING PLOTS ===
# Only run this if you have separate lists for initial training!
epochs_range = range(1, len(train_losses) + 1)

plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
plt.plot(epochs_range, train_losses, label='Train Loss')
plt.plot(epochs_range, val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Initial Training Loss Curve')

plt.subplot(1,2,2)
plt.plot(epochs_range, val_top1, label='Val Top-1 Acc')
plt.plot(epochs_range, val_f1, label='Val F1')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.legend()
plt.title('Initial Validation Accuracy & F1 Curve')

plt.tight_layout()
plt.show()

# === FINE-TUNING PLOTS ===
# Use these if you did fine-tuning and have lists ft_train_losses, ft_val_losses, etc.
ft_epochs_range = range(1, len(ft_train_losses) + 1)

plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
plt.plot(ft_epochs_range, ft_train_losses, label='Fine-tune Train Loss')
plt.plot(ft_epochs_range, ft_val_losses, label='Fine-tune Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Fine-tune Loss Curve')

plt.subplot(1,2,2)
plt.plot(ft_epochs_range, ft_val_top1, label='Fine-tune Val Top-1 Acc')
plt.plot(ft_epochs_range, ft_val_f1, label='Fine-tune Val F1')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.legend()
plt.title('Fine-tune Val Accuracy & F1 Curve')

plt.tight_layout()
plt.show()

# === CONFUSION MATRIX (Validation set) ===

model.load_state_dict(torch.load("best_model.pth"))
model.eval()

all_preds = []
all_labels = []

with torch.no_grad():
    for imgs, labels in val_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model(imgs)
        preds = outputs.argmax(dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=list(full_dataset.class_to_idx.keys()),
            yticklabels=list(full_dataset.class_to_idx.keys()))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Confusion Matrix (Validation Set)')
plt.show()

# === CLASSIFICATION REPORT ===

print(classification_report(all_labels, all_preds, target_names=list(full_dataset.class_to_idx.keys())))

from google.colab import files
uploaded = files.upload()
test_img_path = next(iter(uploaded))
print("Test image path:", test_img_path)

# If you want to upload an image in Colab:
from google.colab import files
uploaded = files.upload()
test_img_path = next(iter(uploaded))  # gets the filename of uploaded image

from inference_sdk import InferenceHTTPClient
import cv2
from PIL import Image
import matplotlib.pyplot as plt

# Make sure model, device, and idx_to_class are defined from above

# Roboflow detection client
rf_client = InferenceHTTPClient(
    api_url="https://serverless.roboflow.com",
    api_key="w6De0OVZmVuuNawaux95"
)

# Transform for face classifier
face_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

img_bgr = cv2.imread(test_img_path)
img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

result = rf_client.run_workflow(
    workspace_name="computer-vision-axoua",
    workflow_id="custom-workflow-3",
    images={"image": test_img_path},
    use_cache=True
)
preds = result[0]['predictions']['predictions']

for prediction in preds:
    x, y = prediction['x'], prediction['y']
    w, h = prediction['width'], prediction['height']
    gender_label = prediction['class']

    x1, y1 = int(max(0, x - w/2)), int(max(0, y - h/2))
    x2, y2 = int(min(img_bgr.shape[1], x + w/2)), int(min(img_bgr.shape[0], y + h/2))

    if y2 > y1 and x2 > x1:
        face_crop = img_rgb[y1:y2, x1:x2]
        face_pil = Image.fromarray(face_crop)
        face_tensor = face_transform(face_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(face_tensor)
            pred_idx = output.argmax(dim=1).item()
            identity_label = idx_to_class[pred_idx]

        color = (0, 0, 255) if "male" in gender_label.lower() else (0, 255, 0)
        display_text = f"{gender_label} | {identity_label}"

        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color, 2)
        cv2.putText(img_rgb, display_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
    else:
        print(f"Skipped invalid crop at box: {(x1, y1, x2, y2)}")

plt.figure(figsize=(10,8))
plt.imshow(img_rgb)
plt.axis('off')
plt.show()

for prediction in preds:
    x, y = prediction['x'], prediction['y']
    w, h = prediction['width'], prediction['height']
    gender_label = prediction['class']

    x1, y1 = int(max(0, x - w/2)), int(max(0, y - h/2))
    x2, y2 = int(min(img_bgr.shape[1], x + w/2)), int(min(img_bgr.shape[0], y + h/2))

    if y2 > y1 and x2 > x1:
        face_crop = img_rgb[y1:y2, x1:x2]
        face_pil = Image.fromarray(face_crop)
        face_tensor = face_transform(face_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(face_tensor)
            pred_idx = output.argmax(dim=1).item()
            identity_label = idx_to_class[pred_idx]

        color = (0, 0, 255) if "male" in gender_label.lower() else (0, 255, 0)
        display_text = f"{gender_label} | {identity_label}"

        print(f"Detected face: Gender: {gender_label}, Predicted Label: {identity_label}")

        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color, 2)
        cv2.putText(img_rgb, display_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
    else:
        print(f"Skipped invalid crop at box: {(x1, y1, x2, y2)}")